{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: running in localhost\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mspatches\n",
    "from mpl_toolkits import mplot3d\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import math\n",
    "import random\n",
    "import json\n",
    "%matplotlib inline\n",
    "# from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "\n",
    "    drive.mount('/content/gdrive')\n",
    "\n",
    "    %tensorflow_version 1.x\n",
    "\n",
    "    colab_running = True\n",
    "except:\n",
    "    print(\"Warning: running in localhost\")\n",
    "\n",
    "    colab_running = False\n",
    "\n",
    "import tensorflow as tf #import the tensorflow library\n",
    "from tensorflow.keras import datasets, layers, models #import the keras library from tensorflow\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights_and_biases(x_train, epochs, loss_function): \n",
    "    \n",
    "    weights_trained = np.load(\"Toy_\" + str(x_train.shape[1]) + \"d_epochs_\" + str(epochs) + \"_loss_\" + loss_function + \"_Weights.npy\", allow_pickle=True).tolist()\n",
    "    for j in range(len(weights_trained)):\n",
    "        weights_trained[j] = tf.Variable(weights_trained[j])\n",
    "    bias_trained = np.load(\"Toy_\" + str(x_train.shape[1]) + \"d_epochs_\" + str(epochs) + \"_loss_\" + loss_function + \"_Bias.npy\", allow_pickle=True).tolist()\n",
    "    for j in range(len(bias_trained)):\n",
    "        bias_trained[j] = tf.Variable(bias_trained[j])\n",
    "        \n",
    "    return weights_trained, bias_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Toy_NN(input_x, weights, biases):\n",
    "    \n",
    "    dens1 = tf.add(tf.matmul(input_x, weights[0]), biases[0])\n",
    "    dens2 = tf.nn.relu(dens1)\n",
    "    dens3 = tf.add(tf.matmul(dens2, weights[1]), biases[1])\n",
    "    \n",
    "    return dens3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_dim_data(dim):\n",
    "    theta = np.random.uniform(0, 2*np.pi, size=(2000))\n",
    "    \n",
    "    x1 = np.array([np.array([np.cos(theta[i])*3, np.sin(theta[i])*3]) for i in range(len(theta))])\n",
    "    while x1.shape[len(x1.shape)-1] != dim:\n",
    "      x1 = np.concatenate((x1, np.zeros((x1.shape[0], 1))), axis=1)\n",
    "    y1 = np.concatenate((np.ones((2000,1)),np.zeros((2000,1))), axis=1)\n",
    "    \n",
    "    theta = np.random.uniform(0, 2*np.pi, size=(2000))\n",
    "    \n",
    "    x2 = np.array([np.array([np.cos(theta[i])*1, np.sin(theta[i])*1]) for i in range(len(theta))])\n",
    "    while x2.shape[len(x2.shape)-1] != dim:\n",
    "      x2 = np.concatenate((x2, np.zeros((x2.shape[0], 1))), axis=1)\n",
    "    y2 = np.concatenate((np.zeros((2000,1)),np.ones((2000,1))), axis=1)\n",
    "    \n",
    "    x_train = np.concatenate((x1,x2), axis=0)\n",
    "    y_train = np.concatenate((y1,y2), axis=0)\n",
    "    shuffle = np.concatenate((x_train, y_train), axis=1)\n",
    "    np.random.shuffle(shuffle)\n",
    "    \n",
    "    x_train = shuffle[2000:,:dim]\n",
    "    x_test = shuffle[:2000,:dim]\n",
    "    \n",
    "    y_train = shuffle[2000:,-2:]\n",
    "    y_test = shuffle[:2000,-2:]\n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_distilled_temperature(x_train, y_train, x_test=None, y_test=None, epochs=250, loss_function=\"CCE\", print_loss=True):\n",
    "    \n",
    "    weights = [tf.Variable(tf.random_normal([x_train.shape[1], 100], stddev=0.1)),\n",
    "          tf.Variable(tf.random_normal([100, y_train.shape[1]], stddev=0.1))]\n",
    "\n",
    "    biases = [tf.Variable(tf.random_normal([100], stddev=0.1)),\n",
    "          tf.Variable(tf.random_normal([y_train.shape[1]], stddev=0.1))]\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=(None, x_train.shape[1]))\n",
    "    y = tf.placeholder(tf.float32, shape=(None, y_train.shape[1]))\n",
    "\n",
    "    logit = Toy_NN(x, weights, biases)\n",
    "\n",
    "    prediction = tf.nn.softmax(logit)\n",
    "\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=logit))\n",
    "    \n",
    "    lr = 0.1\n",
    "    train_op = tf.train.GradientDescentOptimizer(learning_rate=lr).minimize(loss_op)\n",
    "\n",
    "    correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    batch_size = 50\n",
    "\n",
    "    model_weights_trained = \"Toy_\" + str(x_train.shape[1]) + \"d_epochs_\" + str(epochs) + \"_loss_\" + loss_function + \"_Weights\"\n",
    "    model_bias_trained = \"Toy_\" + str(x_train.shape[1]) + \"d_epochs_\" + str(epochs) + \"_loss_\" + loss_function + \"_Bias\"\n",
    "\n",
    "    acc_train_list = list()\n",
    "    acc_test_list = list()\n",
    "    loss_train_list = list()\n",
    "    loss_test_list = list()\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # print(sess.run(weights['wc1']))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            if epoch % 100 == 0 and epoch != 0:\n",
    "                lr = lr/10\n",
    "\n",
    "            for j in range(int(len(x_train)//batch_size)):\n",
    "                \n",
    "                image_batch = x_train[j*batch_size : min((j+1)*batch_size, len(x_train))]\n",
    "                \n",
    "                label_batch = y_train[j*batch_size : min((j+1)*batch_size, len(y_train))]\n",
    "\n",
    "                # print(sess.run(loss_op, feed_dict={x: image_batch, y: label_batch}))\n",
    "\n",
    "                sess.run(train_op, feed_dict={x: image_batch, y: label_batch})\n",
    "\n",
    "            # print(sess.run(weights['wc1']))\n",
    "\n",
    "            acc_train = 0\n",
    "            loss_train = 0\n",
    "\n",
    "            for j in range(10):\n",
    "\n",
    "                image_batch = x_train[j*len(x_train)//10 : min((j+1)*len(x_train)//10, len(x_train))]\n",
    "                label_batch = y_train[j*len(y_train)//10 : min((j+1)*len(y_train)//10, len(y_train))]\n",
    "\n",
    "                loss_train += sess.run(loss_op, feed_dict={x: image_batch, y: label_batch})\n",
    "                acc_train += sess.run(accuracy, feed_dict={x: image_batch, y: label_batch})\n",
    "            \n",
    "            loss_test = sess.run(loss_op, feed_dict={x: x_test, y: y_test})\n",
    "            acc_test = sess.run(accuracy, feed_dict={x: x_test, y: y_test})\n",
    "\n",
    "            acc_train_list.append(acc_train/10)\n",
    "            acc_test_list.append(acc_test)\n",
    "            loss_train_list.append(loss_train/10)\n",
    "            loss_test_list.append(loss_test)\n",
    "            \n",
    "            if print_loss == True:\n",
    "                \n",
    "                print(\"EPOCH: \" + str(epoch+1))\n",
    "\n",
    "                print(\" train loss: \" + str(loss_train/10) + \" train acc: \" + str(acc_train/10) + \" test loss: \" + str(loss_test) + \" test acc: \" + str(acc_test))\n",
    "\n",
    "        weights_after_train = sess.run(weights)\n",
    "        biases_after_train = sess.run(biases)\n",
    "        \n",
    "    np.save(model_weights_trained + \".npy\", weights_after_train)\n",
    "    np.save(model_bias_trained + \".npy\", biases_after_train)\n",
    "    \n",
    "    return acc_train_list, acc_test_list, loss_train_list, loss_test_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_after_training(acc_train, acc_test, loss_train, loss_test):\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(acc_train)\n",
    "    plt.plot(acc_test)\n",
    "    plt.legend(['Train', 'Test'])\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(loss_train)\n",
    "    plt.plot(loss_test)\n",
    "    plt.legend(['Train', 'Test'])\n",
    "    plt.title(\"Loss\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3-dimensional Input Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = generate_n_dim_data(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    acc_train, acc_test, loss_train, loss_test = train_distilled_temperature(x_train, y_train, x_test, y_test, 250, \"CCE_%i\"%i, print_loss=False)\n",
    "    plot_after_training(acc_train, acc_test, loss_train, loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-dimensional Input Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = generate_n_dim_data(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    acc_train, acc_test, loss_train, loss_test = train_distilled_temperature(x_train, y_train, x_test, y_test, 250, \"CCE_%i\"%i, print_loss=False)\n",
    "    plot_after_training(acc_train, acc_test, loss_train, loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 50-dimensional Input Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = generate_n_dim_data(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    acc_train, acc_test, loss_train, loss_test = train_distilled_temperature(x_train, y_train, x_test, y_test, 250, \"CCE_%i\"%i, print_loss=False)\n",
    "    plot_after_training(acc_train, acc_test, loss_train, loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 500-dimensional Input Space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = generate_n_dim_data(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    acc_train, acc_test, loss_train, loss_test = train_distilled_temperature(x_train, y_train, x_test, y_test, 250, \"CCE_%i\"%i, print_loss=False)\n",
    "    plot_after_training(acc_train, acc_test, loss_train, loss_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pertubation Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_visualization(x_input, y_input, epochs=500, loss_function=\"CCE\"):\n",
    "\n",
    "    weights_trained, bias_trained = load_weights_and_biases(x_input, epochs, loss_function)\n",
    "\n",
    "    x = tf.placeholder(tf.float32, shape=(None, x_input.shape[1]))\n",
    "    y = tf.placeholder(tf.float32, shape=(None, y_input.shape[1]))\n",
    "\n",
    "    logit = Toy_NN(x, weights_trained, bias_trained)\n",
    "    prediction = tf.nn.softmax(logit)\n",
    "\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=prediction))\n",
    "\n",
    "    gradients = tf.gradients(loss_op, x)\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        if len(x_input.shape) < 2:\n",
    "\n",
    "            gradient = sess.run(gradients, feed_dict={x:x_input.reshape(1, x_input.shape[0]), y:y_input.reshape(1, y_input.shape[0])})\n",
    "\n",
    "        else:\n",
    "\n",
    "            gradient = sess.run(gradients, feed_dict={x:x_input, y:y_input})\n",
    "\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_visualization_grad_proj(dim, manifold_dim, norm=\"INF\", perturbation=None, epochs=500, loss_function=\"CCE\"):\n",
    "\n",
    "    theta = angle_calculation_grad_proj(dim, manifold_dim, norm, perturbation, epochs, loss_function)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.title(\"Angle Visualization\", fontsize=15)\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.hist((theta[:1000]/math.pi)*180, bins=50, range=(0, 180), rwidth=0.9)\n",
    "    plt.xlabel(\"$\\phi$\", fontsize=15)\n",
    "    plt.axis([0, 180, 0, 1000])\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.hist((theta[1000:]/math.pi)*180, bins=50, range=(0, 180), rwidth=0.9)\n",
    "    plt.xlabel(\"$\\phi$\", fontsize=15)\n",
    "    plt.axis([0, 180, 0, 1000])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def angle_calculation_grad_proj(dim, manifold_dim, norm=\"INF\", perturbation=None, epochs=500, loss_function=\"CCE\"):\n",
    "    x_manifold, y_manifold = data_manifold(dim)\n",
    "    if perturbation == None:\n",
    "        gradients = gradient_visualization(x_manifold, y_manifold, epochs, loss_function)\n",
    "    else:\n",
    "        gradients = perturbation\n",
    "\n",
    "    if norm == \"INF\":\n",
    "        gradients = np.sign(gradients)\n",
    "    elif norm == 2:\n",
    "        gradients = gradients/np.sqrt(np.maximum(1e-20, np.sum(np.square(gradients))))\n",
    "    else:\n",
    "        print(\"Implemented only norms 2 and $\\infty$\")\n",
    "\n",
    "    normal_space_dim = range(dim)\n",
    "    normal_space_dim = np.delete(normal_space_dim, manifold_dim)\n",
    "\n",
    "    np.asarray(gradients[0]).shape\n",
    "\n",
    "    grad_proj = np.zeros(np.asarray(gradients[0]).shape)\n",
    "    grad_proj[:, normal_space_dim] =  np.asarray(gradients[0])[:, normal_space_dim]\n",
    "\n",
    "    dot_prod = list()\n",
    "    for j in range(len(grad_proj)):\n",
    "        dot_prod.append(np.sum(gradients[0][j]*grad_proj[j]))\n",
    "\n",
    "    cos_theta = dot_prod/(np.sqrt(np.sum(np.square(gradients[0]), axis=1))*np.sqrt(np.sum(np.square(grad_proj), axis=1)))\n",
    "    theta = np.arccos(np.clip(cos_theta, -1, 1))\n",
    "\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_manifold(dim):\n",
    "\n",
    "    theta1 = np.linspace(0, 2*np.pi, 1000)\n",
    "    theta2 = np.linspace(0, 2*np.pi, 1000)\n",
    "\n",
    "    x1 = np.array([np.array([np.cos(theta1[i])*3, np.sin(theta1[i])*3]) for i in range(len(theta1))])\n",
    "    while x1.shape[len(x1.shape)-1] != dim:\n",
    "      x1 = np.concatenate((x1, np.zeros((x1.shape[0], 1))), axis=1)\n",
    "    y1 = np.concatenate((np.ones((1000,1)),np.zeros((1000,1))), axis=1)\n",
    "\n",
    "    x2 = np.array([np.array([np.cos(theta1[i])*1, np.sin(theta1[i])*1]) for i in range(len(theta1))])\n",
    "    while x2.shape[len(x2.shape)-1] != dim:\n",
    "      x2 = np.concatenate((x2, np.zeros((x2.shape[0], 1))), axis=1)\n",
    "    y2 = np.concatenate((np.zeros((1000,1)),np.ones((1000,1))), axis=1)\n",
    "\n",
    "    x_manifold = np.concatenate((x1,x2), axis=0)\n",
    "    y_manifold = np.concatenate((y1,y2), axis=0)\n",
    "\n",
    "    return x_manifold, y_manifold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_list = list()\n",
    "for i in range(20):\n",
    "    theta_list.append(angle_calculation_grad_proj(3, [0,1], norm=2, epochs=250, loss_function=\"CCE_\" + str(i)))\n",
    "#     angle_visualization_grad_proj(3, [0,1], norm=2, epochs=250, loss_function=\"CCE_\" + str(i+1))\n",
    "    \n",
    "\n",
    "plt.hist((np.asarray(theta_list[:len(theta_list)//2]).reshape(len(theta_list)//2*len(theta_list[0]))/math.pi)*180, range=(0, 180), bins=50)\n",
    "plt.show()\n",
    "\n",
    "plt.hist((np.asarray(theta_list[len(theta_list)//2:]).reshape(len(theta_list)//2*len(theta_list[0]))/math.pi)*180, range=(0, 180), bins=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_list = list()\n",
    "for i in range(20):\n",
    "    theta_list.append(angle_calculation_grad_proj(10, [0,1], norm=2, epochs=250, loss_function=\"CCE_\" + str(i)))\n",
    "#     angle_visualization_grad_proj(3, [0,1], norm=2, epochs=250, loss_function=\"CCE_\" + str(i+1))\n",
    "    \n",
    "\n",
    "plt.hist((np.asarray(theta_list[:len(theta_list)//2]).reshape(len(theta_list)//2*len(theta_list[0]))/math.pi)*180, range=(0, 180), bins=50)\n",
    "plt.show()\n",
    "\n",
    "plt.hist((np.asarray(theta_list[len(theta_list)//2:]).reshape(len(theta_list)//2*len(theta_list[0]))/math.pi)*180, range=(0, 180), bins=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_list = list()\n",
    "for i in range(20):\n",
    "    theta_list.append(angle_calculation_grad_proj(50, [0,1], norm=2, epochs=250, loss_function=\"CCE_\" + str(i)))\n",
    "#     angle_visualization_grad_proj(3, [0,1], norm=2, epochs=250, loss_function=\"CCE_\" + str(i+1))\n",
    "    \n",
    "\n",
    "plt.hist((np.asarray(theta_list[:len(theta_list)//2]).reshape(len(theta_list)//2*len(theta_list[0]))/math.pi)*180, range=(0, 180), bins=50)\n",
    "plt.show()\n",
    "\n",
    "plt.hist((np.asarray(theta_list[len(theta_list)//2:]).reshape(len(theta_list)//2*len(theta_list[0]))/math.pi)*180, range=(0, 180), bins=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_list = list()\n",
    "for i in range(20):\n",
    "    theta_list.append(angle_calculation_grad_proj(500, [0,1], norm=2, epochs=250, loss_function=\"CCE_\" + str(i)))\n",
    "#     angle_visualization_grad_proj(3, [0,1], norm=2, epochs=250, loss_function=\"CCE_\" + str(i+1))\n",
    "    \n",
    "\n",
    "plt.hist((np.asarray(theta_list[:len(theta_list)//2]).reshape(len(theta_list)//2*len(theta_list[0]))/math.pi)*180, range=(0, 180), bins=50)\n",
    "plt.show()\n",
    "\n",
    "plt.hist((np.asarray(theta_list[len(theta_list)//2:]).reshape(len(theta_list)//2*len(theta_list[0]))/math.pi)*180, range=(0, 180), bins=50)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
